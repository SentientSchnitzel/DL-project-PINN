training:
  learning_rate: [0.1]   #! list
  epochs: 3000
  optimizer: AdamW # either AdamW, Adam or SGD
  L2_penalty: 0.005 # set 0.005 for L2 penalty
  hidden_dim: [64]        #! list
  n_layers: [1]           #! list
  dropout_rate: [0.0] # set to 0.0 for no dropout #! list
  scheduler: [CAWR] # set none for no scheduler, options [ OneCycleLR, CAWR (CosineAnnealingWarmRestarts), Cycl (CyclicLR) ] #! list
  adaptive: true
  initialization: true # always true
  patience: 500

logging:
  tensorboard: false
  n_logs: 500 # number of times there is a log of loss etc

data:
  batch_size: 32 # set-1 for no minibatching
  t_domain: [0,2]
  x_domain: [-1,1]
  num_collocation_points: 2000

physics:
  wave_speed: 1.0
  sigma: 0.2
  x0: 0.2
  amplitude: 1.0