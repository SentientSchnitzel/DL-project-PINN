{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Her starter PINN delen\n",
    "\n",
    "vi kunne godt tænke os en løsning til følgende problem\n",
    "\n",
    "$$u''(t) = 0 \\,\\,\\, u(0) = 1\\,\\,\\, u'(1) = 2$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "def u_exact(t):\n",
    "    return(2*t+1)\n",
    "\n",
    "def u_dif_dif(t):\n",
    "\n",
    "    return(torch.zeros(len(t)).view(-1,1) )\n",
    "\n",
    "\n",
    "boundary = {'BC1': torch.tensor(1), 'BC2':torch.tensor(2)} \n",
    "\n",
    "\n",
    "\n",
    "##vi mangler u_diff til boundary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_domain(t): #input vektor af tidpunkter\n",
    "\n",
    "    u_pred = model(t)  #fx 10 punkter i y_pred\n",
    "    u_pred_2 = u_pred.mean()\n",
    "    \n",
    "    first_derivative = torch.autograd.grad(u_pred_2, t, create_graph=True)[0]\n",
    "    # We now have dloss/dx\n",
    "    second_derivative = torch.autograd.grad(first_derivative.mean(), t)[0]\n",
    "    # This computes d/dx(dloss/dx) = d2loss/dx2\n",
    "\n",
    "\n",
    "    u_pred_dif_dif = second_derivative\n",
    "    print(\"Den første \", (u_pred_dif_dif))\n",
    "    print(\"Den anden \", (u_dif_dif(t)))\n",
    "    \n",
    "    print(\"Den næste: \",((u_pred_dif_dif-u_dif_dif(t))**2))\n",
    "    return torch.mean((u_pred_dif_dif-u_dif_dif(t))**2)\n",
    "\n",
    "def loss_boundary(t): #TODO : refaktor vektor\n",
    "    \n",
    "\n",
    "    u_pred_0 = model(torch.tensor(0.,requires_grad=True).view(-1,1))\n",
    "    \n",
    "    t1 = torch.tensor([0.,1.,2.] ,requires_grad=True).view(-1,1)\n",
    "    u_pred_1 = model(t1)\n",
    "    print(u_pred_1)\n",
    "\n",
    "    \n",
    "    u_pred_1 = torch.autograd.grad(u_pred_1, t1, create_graph=False,allow_unused=True)[0]\n",
    "    print(u_pred_0)\n",
    "    print(u_pred_1)\n",
    "    loss = 1/2*((u_pred_0 - boundary['BC1'])**2 + (u_pred_1- boundary['BC2'])**2)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def combine_loss(t):\n",
    "\n",
    "    return loss_domain(t)+loss_boundary(t)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PINN, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(1, 64), # Input layer with 1 input and 64 neurons\n",
    "            # nn.LeakyReLU(),        # Activation function\n",
    "            # nn.Tanh(),\n",
    "            # nn.ReLU(),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(64, 64),# Hidden layer with 64 neurons\n",
    "            # nn.LeakyReLU(),        # Activation function\n",
    "            # nn.Tanh(),\n",
    "            # nn.ReLU(),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(64, 1)  # Output layer with 1 output\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.linspace(-2, 5, 200).view(-1,1)\n",
    "x_train.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "model = PINN()\n",
    "\n",
    "# Loss function\n",
    "criterion = combine_loss\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Den første  tensor([[ 6.8513e-08],\n",
      "        [ 6.1278e-08],\n",
      "        [ 5.3826e-08],\n",
      "        [ 4.6151e-08],\n",
      "        [ 3.8244e-08],\n",
      "        [ 3.0096e-08],\n",
      "        [ 2.1700e-08],\n",
      "        [-8.7018e-08],\n",
      "        [-9.3496e-08],\n",
      "        [-1.0020e-07],\n",
      "        [-1.6146e-07],\n",
      "        [-1.7157e-07],\n",
      "        [-1.8205e-07],\n",
      "        [-1.1486e-07],\n",
      "        [-1.3137e-07],\n",
      "        [-1.4833e-07],\n",
      "        [-2.4146e-07],\n",
      "        [-2.6016e-07],\n",
      "        [-2.7931e-07],\n",
      "        [-2.9895e-07],\n",
      "        [-2.7383e-07],\n",
      "        [-2.9202e-07],\n",
      "        [-3.2357e-07],\n",
      "        [-3.4195e-07],\n",
      "        [-5.1596e-07],\n",
      "        [-5.2222e-07],\n",
      "        [-5.3653e-07],\n",
      "        [-5.4785e-07],\n",
      "        [-1.3371e-06],\n",
      "        [-1.2101e-06],\n",
      "        [-1.2140e-06],\n",
      "        [-1.2188e-06],\n",
      "        [-1.5393e-06],\n",
      "        [-1.5411e-06],\n",
      "        [-1.4946e-06],\n",
      "        [-1.4670e-06],\n",
      "        [-1.4130e-06],\n",
      "        [-1.3846e-06],\n",
      "        [-1.3571e-06],\n",
      "        [-1.3324e-06],\n",
      "        [-1.3065e-06],\n",
      "        [-4.2110e-07],\n",
      "        [-3.9062e-07],\n",
      "        [-4.7545e-07],\n",
      "        [-9.1849e-07],\n",
      "        [-9.0100e-07],\n",
      "        [-1.0761e-06],\n",
      "        [ 7.5486e-07],\n",
      "        [ 6.0164e-07],\n",
      "        [ 7.0289e-07],\n",
      "        [ 7.3744e-07],\n",
      "        [ 7.7106e-07],\n",
      "        [ 3.4038e-07],\n",
      "        [ 3.7674e-07],\n",
      "        [ 4.4296e-07],\n",
      "        [ 8.4240e-07],\n",
      "        [ 8.7993e-07],\n",
      "        [ 7.7343e-07],\n",
      "        [ 8.1290e-07],\n",
      "        [ 7.6340e-07],\n",
      "        [ 7.7882e-07],\n",
      "        [ 8.1825e-07],\n",
      "        [ 1.6309e-06],\n",
      "        [ 1.5459e-06],\n",
      "        [ 1.5965e-06],\n",
      "        [ 1.4936e-06],\n",
      "        [ 1.7712e-06],\n",
      "        [ 1.7765e-06],\n",
      "        [ 1.7457e-06],\n",
      "        [ 1.4683e-06],\n",
      "        [ 1.4465e-06],\n",
      "        [ 1.3187e-06],\n",
      "        [ 1.2999e-06],\n",
      "        [ 1.3099e-06],\n",
      "        [ 6.9223e-07],\n",
      "        [ 6.7706e-07],\n",
      "        [ 6.6212e-07],\n",
      "        [ 5.4062e-08],\n",
      "        [ 4.5628e-08],\n",
      "        [-3.4753e-08],\n",
      "        [-4.4158e-08],\n",
      "        [-5.3461e-08],\n",
      "        [-1.5558e-07],\n",
      "        [-4.4867e-07],\n",
      "        [-4.6440e-07],\n",
      "        [-4.8006e-07],\n",
      "        [-4.9568e-07],\n",
      "        [-3.4134e-08],\n",
      "        [-1.3566e-07],\n",
      "        [-1.9122e-07],\n",
      "        [-1.8586e-07],\n",
      "        [-5.6185e-07],\n",
      "        [-3.8166e-08],\n",
      "        [-2.4752e-08],\n",
      "        [-1.1931e-08],\n",
      "        [-1.9405e-09],\n",
      "        [ 1.7864e-08],\n",
      "        [ 2.9208e-08],\n",
      "        [-1.7347e-08],\n",
      "        [-7.1183e-09],\n",
      "        [ 2.7084e-09],\n",
      "        [ 1.2156e-08],\n",
      "        [ 2.1248e-08],\n",
      "        [ 3.0004e-08],\n",
      "        [ 3.8446e-08],\n",
      "        [ 4.6591e-08],\n",
      "        [ 4.4937e-07],\n",
      "        [ 4.4709e-07],\n",
      "        [ 2.9222e-07],\n",
      "        [ 2.9843e-07],\n",
      "        [ 3.0439e-07],\n",
      "        [ 3.1691e-07],\n",
      "        [ 3.2226e-07],\n",
      "        [ 2.6594e-07],\n",
      "        [ 2.6999e-07],\n",
      "        [ 2.7387e-07],\n",
      "        [ 9.9453e-08],\n",
      "        [ 1.0092e-07],\n",
      "        [ 8.0736e-08],\n",
      "        [ 1.0693e-07],\n",
      "        [ 1.0771e-07],\n",
      "        [ 9.6793e-08],\n",
      "        [ 9.7400e-08],\n",
      "        [-1.0446e-07],\n",
      "        [-1.1559e-07],\n",
      "        [-1.1054e-07],\n",
      "        [-1.0575e-07],\n",
      "        [-1.0122e-07],\n",
      "        [-9.6921e-08],\n",
      "        [-9.2850e-08],\n",
      "        [-8.8993e-08],\n",
      "        [-8.5339e-08],\n",
      "        [-8.1877e-08],\n",
      "        [-1.0461e-07],\n",
      "        [-1.0123e-07],\n",
      "        [-9.8040e-08],\n",
      "        [-9.5016e-08],\n",
      "        [-9.2156e-08],\n",
      "        [-8.9163e-08],\n",
      "        [-8.6580e-08],\n",
      "        [-8.4137e-08],\n",
      "        [-8.1825e-08],\n",
      "        [-7.9639e-08],\n",
      "        [-7.7572e-08],\n",
      "        [-7.4136e-08],\n",
      "        [-7.2288e-08],\n",
      "        [-3.0188e-08],\n",
      "        [-2.9007e-08],\n",
      "        [-2.7912e-08],\n",
      "        [-2.6901e-08],\n",
      "        [-3.7144e-08],\n",
      "        [-3.6205e-08],\n",
      "        [-3.5341e-08],\n",
      "        [-3.4547e-08],\n",
      "        [-3.3819e-08],\n",
      "        [-3.3155e-08],\n",
      "        [-3.2549e-08],\n",
      "        [ 1.7158e-08],\n",
      "        [ 1.8487e-08],\n",
      "        [ 1.9761e-08],\n",
      "        [ 2.0985e-08],\n",
      "        [ 2.2161e-08],\n",
      "        [ 2.3291e-08],\n",
      "        [ 4.1572e-08],\n",
      "        [ 4.2789e-08],\n",
      "        [ 4.3967e-08],\n",
      "        [ 4.5108e-08],\n",
      "        [ 4.6215e-08],\n",
      "        [ 4.7290e-08],\n",
      "        [ 4.8335e-08],\n",
      "        [ 4.9351e-08],\n",
      "        [ 4.9944e-08],\n",
      "        [ 5.2729e-08],\n",
      "        [ 5.3784e-08],\n",
      "        [ 5.4816e-08],\n",
      "        [ 5.5828e-08],\n",
      "        [ 5.6820e-08],\n",
      "        [ 5.7794e-08],\n",
      "        [ 5.8751e-08],\n",
      "        [ 5.9693e-08],\n",
      "        [ 5.3138e-08],\n",
      "        [ 5.3834e-08],\n",
      "        [ 5.4518e-08],\n",
      "        [ 5.5192e-08],\n",
      "        [ 5.5856e-08],\n",
      "        [ 5.6512e-08],\n",
      "        [ 5.7159e-08],\n",
      "        [ 5.7799e-08],\n",
      "        [ 5.8433e-08],\n",
      "        [ 5.9061e-08],\n",
      "        [ 5.9684e-08],\n",
      "        [ 6.0302e-08],\n",
      "        [ 6.0916e-08],\n",
      "        [ 6.1527e-08],\n",
      "        [ 6.2135e-08],\n",
      "        [ 6.2741e-08],\n",
      "        [ 6.3345e-08],\n",
      "        [ 6.3947e-08],\n",
      "        [ 6.4548e-08],\n",
      "        [ 6.5148e-08]])\n",
      "Den anden  tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "Den næste:  tensor([[4.6940e-15],\n",
      "        [3.7549e-15],\n",
      "        [2.8973e-15],\n",
      "        [2.1300e-15],\n",
      "        [1.4626e-15],\n",
      "        [9.0580e-16],\n",
      "        [4.7089e-16],\n",
      "        [7.5721e-15],\n",
      "        [8.7415e-15],\n",
      "        [1.0039e-14],\n",
      "        [2.6071e-14],\n",
      "        [2.9438e-14],\n",
      "        [3.3142e-14],\n",
      "        [1.3193e-14],\n",
      "        [1.7258e-14],\n",
      "        [2.2003e-14],\n",
      "        [5.8304e-14],\n",
      "        [6.7681e-14],\n",
      "        [7.8015e-14],\n",
      "        [8.9369e-14],\n",
      "        [7.4981e-14],\n",
      "        [8.5273e-14],\n",
      "        [1.0470e-13],\n",
      "        [1.1693e-13],\n",
      "        [2.6622e-13],\n",
      "        [2.7272e-13],\n",
      "        [2.8787e-13],\n",
      "        [3.0014e-13],\n",
      "        [1.7878e-12],\n",
      "        [1.4643e-12],\n",
      "        [1.4738e-12],\n",
      "        [1.4854e-12],\n",
      "        [2.3696e-12],\n",
      "        [2.3749e-12],\n",
      "        [2.2338e-12],\n",
      "        [2.1520e-12],\n",
      "        [1.9965e-12],\n",
      "        [1.9171e-12],\n",
      "        [1.8418e-12],\n",
      "        [1.7752e-12],\n",
      "        [1.7070e-12],\n",
      "        [1.7732e-13],\n",
      "        [1.5258e-13],\n",
      "        [2.2606e-13],\n",
      "        [8.4363e-13],\n",
      "        [8.1180e-13],\n",
      "        [1.1579e-12],\n",
      "        [5.6981e-13],\n",
      "        [3.6197e-13],\n",
      "        [4.9405e-13],\n",
      "        [5.4382e-13],\n",
      "        [5.9454e-13],\n",
      "        [1.1586e-13],\n",
      "        [1.4193e-13],\n",
      "        [1.9621e-13],\n",
      "        [7.0964e-13],\n",
      "        [7.7427e-13],\n",
      "        [5.9819e-13],\n",
      "        [6.6081e-13],\n",
      "        [5.8279e-13],\n",
      "        [6.0657e-13],\n",
      "        [6.6954e-13],\n",
      "        [2.6598e-12],\n",
      "        [2.3898e-12],\n",
      "        [2.5488e-12],\n",
      "        [2.2309e-12],\n",
      "        [3.1372e-12],\n",
      "        [3.1561e-12],\n",
      "        [3.0474e-12],\n",
      "        [2.1559e-12],\n",
      "        [2.0924e-12],\n",
      "        [1.7390e-12],\n",
      "        [1.6896e-12],\n",
      "        [1.7157e-12],\n",
      "        [4.7919e-13],\n",
      "        [4.5841e-13],\n",
      "        [4.3840e-13],\n",
      "        [2.9227e-15],\n",
      "        [2.0819e-15],\n",
      "        [1.2078e-15],\n",
      "        [1.9500e-15],\n",
      "        [2.8581e-15],\n",
      "        [2.4204e-14],\n",
      "        [2.0130e-13],\n",
      "        [2.1567e-13],\n",
      "        [2.3046e-13],\n",
      "        [2.4569e-13],\n",
      "        [1.1651e-15],\n",
      "        [1.8402e-14],\n",
      "        [3.6566e-14],\n",
      "        [3.4543e-14],\n",
      "        [3.1568e-13],\n",
      "        [1.4566e-15],\n",
      "        [6.1268e-16],\n",
      "        [1.4234e-16],\n",
      "        [3.7656e-18],\n",
      "        [3.1912e-16],\n",
      "        [8.5309e-16],\n",
      "        [3.0090e-16],\n",
      "        [5.0670e-17],\n",
      "        [7.3355e-18],\n",
      "        [1.4777e-16],\n",
      "        [4.5146e-16],\n",
      "        [9.0024e-16],\n",
      "        [1.4781e-15],\n",
      "        [2.1708e-15],\n",
      "        [2.0194e-13],\n",
      "        [1.9989e-13],\n",
      "        [8.5392e-14],\n",
      "        [8.9063e-14],\n",
      "        [9.2655e-14],\n",
      "        [1.0043e-13],\n",
      "        [1.0385e-13],\n",
      "        [7.0723e-14],\n",
      "        [7.2896e-14],\n",
      "        [7.5003e-14],\n",
      "        [9.8909e-15],\n",
      "        [1.0185e-14],\n",
      "        [6.5182e-15],\n",
      "        [1.1433e-14],\n",
      "        [1.1601e-14],\n",
      "        [9.3688e-15],\n",
      "        [9.4868e-15],\n",
      "        [1.0912e-14],\n",
      "        [1.3360e-14],\n",
      "        [1.2218e-14],\n",
      "        [1.1183e-14],\n",
      "        [1.0245e-14],\n",
      "        [9.3936e-15],\n",
      "        [8.6211e-15],\n",
      "        [7.9197e-15],\n",
      "        [7.2827e-15],\n",
      "        [6.7039e-15],\n",
      "        [1.0943e-14],\n",
      "        [1.0249e-14],\n",
      "        [9.6118e-15],\n",
      "        [9.0281e-15],\n",
      "        [8.4926e-15],\n",
      "        [7.9500e-15],\n",
      "        [7.4961e-15],\n",
      "        [7.0790e-15],\n",
      "        [6.6954e-15],\n",
      "        [6.3424e-15],\n",
      "        [6.0174e-15],\n",
      "        [5.4962e-15],\n",
      "        [5.2256e-15],\n",
      "        [9.1133e-16],\n",
      "        [8.4138e-16],\n",
      "        [7.7911e-16],\n",
      "        [7.2367e-16],\n",
      "        [1.3797e-15],\n",
      "        [1.3108e-15],\n",
      "        [1.2490e-15],\n",
      "        [1.1935e-15],\n",
      "        [1.1437e-15],\n",
      "        [1.0992e-15],\n",
      "        [1.0595e-15],\n",
      "        [2.9440e-16],\n",
      "        [3.4176e-16],\n",
      "        [3.9052e-16],\n",
      "        [4.4037e-16],\n",
      "        [4.9110e-16],\n",
      "        [5.4248e-16],\n",
      "        [1.7282e-15],\n",
      "        [1.8309e-15],\n",
      "        [1.9331e-15],\n",
      "        [2.0347e-15],\n",
      "        [2.1358e-15],\n",
      "        [2.2364e-15],\n",
      "        [2.3363e-15],\n",
      "        [2.4356e-15],\n",
      "        [2.4944e-15],\n",
      "        [2.7803e-15],\n",
      "        [2.8927e-15],\n",
      "        [3.0048e-15],\n",
      "        [3.1168e-15],\n",
      "        [3.2285e-15],\n",
      "        [3.3402e-15],\n",
      "        [3.4517e-15],\n",
      "        [3.5632e-15],\n",
      "        [2.8236e-15],\n",
      "        [2.8981e-15],\n",
      "        [2.9722e-15],\n",
      "        [3.0461e-15],\n",
      "        [3.1199e-15],\n",
      "        [3.1936e-15],\n",
      "        [3.2672e-15],\n",
      "        [3.3408e-15],\n",
      "        [3.4144e-15],\n",
      "        [3.4882e-15],\n",
      "        [3.5622e-15],\n",
      "        [3.6363e-15],\n",
      "        [3.7108e-15],\n",
      "        [3.7856e-15],\n",
      "        [3.8608e-15],\n",
      "        [3.9364e-15],\n",
      "        [4.0125e-15],\n",
      "        [4.0892e-15],\n",
      "        [4.1664e-15],\n",
      "        [4.2443e-15]])\n",
      "tensor([[-0.2892],\n",
      "        [-0.2658],\n",
      "        [-0.2348]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[165], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m      4\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()           \u001b[39m# Clear gradients for the next train\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     loss \u001b[39m=\u001b[39m criterion(x_train) \u001b[39m# Compute loss\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     loss\u001b[39m.\u001b[39mbackward()                 \u001b[39m# Backward pass: Compute gradient of the loss with respect to model parameters\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()                \u001b[39m# Update weights\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[161], line 38\u001b[0m, in \u001b[0;36mcombine_loss\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcombine_loss\u001b[39m(t):\n\u001b[1;32m---> 38\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_domain(t)\u001b[39m+\u001b[39mloss_boundary(t)\n",
      "Cell \u001b[1;32mIn[161], line 29\u001b[0m, in \u001b[0;36mloss_boundary\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m     25\u001b[0m u_pred_1 \u001b[39m=\u001b[39m model(t1)\n\u001b[0;32m     26\u001b[0m \u001b[39mprint\u001b[39m(u_pred_1)\n\u001b[1;32m---> 29\u001b[0m u_pred_1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mgrad(u_pred_1, t1, create_graph\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,allow_unused\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     30\u001b[0m \u001b[39mprint\u001b[39m(u_pred_0)\n\u001b[0;32m     31\u001b[0m \u001b[39mprint\u001b[39m(u_pred_1)\n",
      "File \u001b[1;32mc:\\Users\\esthe\\Desktop\\2. Semester\\Deep Learning\\DL-project-PINN\\.venv\\lib\\site-packages\\torch\\autograd\\__init__.py:367\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[0;32m    360\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    361\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39monly_inputs argument is deprecated and is ignored now \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    362\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(defaults to True). To accumulate gradient for other \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mparts of the graph, please use torch.autograd.backward.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    364\u001b[0m     )\n\u001b[0;32m    366\u001b[0m grad_outputs_ \u001b[39m=\u001b[39m _tensor_or_tensors_to_tuple(grad_outputs, \u001b[39mlen\u001b[39m(t_outputs))\n\u001b[1;32m--> 367\u001b[0m grad_outputs_ \u001b[39m=\u001b[39m _make_grads(\n\u001b[0;32m    368\u001b[0m     t_outputs, grad_outputs_, is_grads_batched\u001b[39m=\u001b[39;49mis_grads_batched\n\u001b[0;32m    369\u001b[0m )\n\u001b[0;32m    371\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    372\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n",
      "File \u001b[1;32mc:\\Users\\esthe\\Desktop\\2. Semester\\Deep Learning\\DL-project-PINN\\.venv\\lib\\site-packages\\torch\\autograd\\__init__.py:117\u001b[0m, in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mrequires_grad:\n\u001b[0;32m    116\u001b[0m     \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mnumel() \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 117\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    118\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    119\u001b[0m         )\n\u001b[0;32m    120\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m out\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mis_floating_point:\n\u001b[0;32m    121\u001b[0m         msg \u001b[39m=\u001b[39m (\n\u001b[0;32m    122\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    123\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m but got \u001b[39m\u001b[39m{\u001b[39;00mout\u001b[39m.\u001b[39mdtype\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    124\u001b[0m         )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "epochs = 4000\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()           # Clear gradients for the next train\n",
    "           \n",
    "    loss = criterion(x_train) # Compute loss\n",
    "    loss.backward()                 # Backward pass: Compute gradient of the loss with respect to model parameters\n",
    "    optimizer.step()                # Update weights\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
