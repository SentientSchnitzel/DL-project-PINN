training:
  learning_rate: 0.01
  epochs: 2000
  optimizer: adam
  hidden_dim: 32
  n_layers: 1
  dropout_rate: 0.0 # set to 0.0 for no dropout
  scheduler: CyclicLR # set none for no scheduler, options [OneCycleLR, CosineAnnealingWarmRestarts, CyclicLR]
  adaptive: true
  initialization: true

logging:
  tensorboard: false
  n_logs: 100 # number of times there is a log of loss etc

data:
  batch_size: 32 # set-1 for no minibatching
  t_domain: [0,2]
  x_domain: [-1,1]
  num_collocation_points: 1000

physics:
  wave_speed: 1.0
  sigma: 0.2
  x0: 0.2
  amplitude: 1